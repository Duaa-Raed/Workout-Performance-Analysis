{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN4N391XSNovi/bfnfMW1Io",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Duaa-Raed/Workout-Performance-Analysis/blob/main/Workout_Performance_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nh_cE5UandZ_"
      },
      "outputs": [],
      "source": [
        "# Install kaggle API\n",
        "!pip install kaggle\n",
        "\n",
        "# Create a directory to store kaggle.json\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "# Upload kaggle.json from your device\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move kaggle.json to the correct folder and set permissions\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "QJ-O4aBgohla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download data from Kaggle\n",
        "!kaggle datasets download -d hasyimabdillah/workoutfitness-video\n",
        "\n",
        "# Unzip the data\n",
        "!unzip workoutfitness-video.zip -d workout_videos"
      ],
      "metadata": {
        "id": "pux383b7opJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numpy==1.26.4 mediapipe==0.10.14 --force-reinstall"
      ],
      "metadata": {
        "id": "0elHQDYdQs9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import all necessary libraries\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapipe as mp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "metadata": {
        "id": "whlXjrTiEQ9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 10 files in the videos folder\n",
        "\n",
        "files = os.listdir(\"workout_videos\")\n",
        "print(\"Number of videos:\", len(files))\n",
        "print(\"Names of some files:\", files[:5])"
      ],
      "metadata": {
        "id": "1P97KgUnrXxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# List exercise folders inside the main dataset\n",
        "folders = os.listdir(\"workout_videos\")\n",
        "exercise_folder = folders[0]  # for example: \"russian twist\"\n",
        "print(\"Selected exercise folder:\", exercise_folder)\n",
        "\n",
        "# List videos inside that exercise folder\n",
        "path = os.path.join(\"workout_videos\", exercise_folder)\n",
        "videos = os.listdir(path)\n",
        "print(\"Number of videos in this exercise:\", len(videos))\n",
        "print(\"First few videos:\", videos[:5])\n",
        "\n",
        "# Select the first video\n",
        "video_path = os.path.join(path, videos[0])\n",
        "print(\"Opening video:\", video_path)\n",
        "\n",
        "# Read the first frame from the video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "ret, frame = cap.read()\n",
        "cap.release()\n",
        "\n",
        "if not ret:\n",
        "    print(\" Failed to read the video or video is empty.\")\n",
        "else:\n",
        "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(frame)\n",
        "    plt.title(f\"First frame from: {exercise_folder}\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "zLY9K3-JsE43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Base folder\n",
        "base_folder = \"workout_videos\"\n",
        "\n",
        "# Select the first exercise folder\n",
        "exercise_folder = os.path.join(base_folder, os.listdir(base_folder)[0])\n",
        "print(\" Exercise Folder:\", exercise_folder)\n",
        "\n",
        "# Display the first 5 files inside\n",
        "files_inside = os.listdir(exercise_folder)\n",
        "print(\" Files inside:\", files_inside[:5])\n",
        "\n",
        "# Select the first video inside\n",
        "video_path = os.path.join(exercise_folder, files_inside[0])\n",
        "print(\" Video Path:\", video_path)\n",
        "\n",
        "# Try to open the video\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "ret, frame = cap.read()\n",
        "cap.release()\n",
        "\n",
        "if ret:\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    mp_pose = mp.solutions.pose\n",
        "    pose = mp_pose.Pose(static_image_mode=True, min_detection_confidence=0.5)\n",
        "    mp_drawing = mp.solutions.drawing_utils\n",
        "\n",
        "    results = pose.process(frame_rgb)\n",
        "\n",
        "    annotated_image = frame_rgb.copy()\n",
        "    mp_drawing.draw_landmarks(\n",
        "        annotated_image,\n",
        "        results.pose_landmarks,\n",
        "        mp_pose.POSE_CONNECTIONS\n",
        "    )\n",
        "\n",
        "    plt.imshow(annotated_image)\n",
        "    plt.title(\"Pose Estimation Example\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\" Failed to read the video frame — maybe not a video file.\")"
      ],
      "metadata": {
        "id": "Na2GgI0Jt4bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Process the same frame we analyzed before\n",
        "results = pose.process(frame_rgb)\n",
        "\n",
        "landmarks = []\n",
        "if results.pose_landmarks:\n",
        "    for id, lm in enumerate(results.pose_landmarks.landmark):\n",
        "        landmarks.append({\n",
        "            \"id\": id,             # joint id\n",
        "            \"x\": lm.x,            # horizontal coordinate\n",
        "            \"y\": lm.y,            # vertical coordinate\n",
        "            \"z\": lm.z,            # depth (distance from camera)\n",
        "            \"visibility\": lm.visibility  # how visible the joint is\n",
        "        })\n",
        "\n",
        "df = pd.DataFrame(landmarks)\n",
        "print(\" Pose landmarks extracted successfully!\")\n",
        "print(df.head(10))\n"
      ],
      "metadata": {
        "id": "NhAMnVl9uk0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose()\n",
        "\n",
        "data = []  # list to collect all frames' data\n",
        "\n",
        "base_dir = \"workout_videos\"  # folder that contains all exercise folders\n",
        "\n",
        "# Loop through all exercise folders\n",
        "for exercise_name in os.listdir(base_dir):\n",
        "    exercise_path = os.path.join(base_dir, exercise_name)\n",
        "\n",
        "    # Check if it's a folder (not a file)\n",
        "    if not os.path.isdir(exercise_path):\n",
        "        continue\n",
        "\n",
        "    # Loop through all videos in the exercise folder\n",
        "    for video_file in os.listdir(exercise_path):\n",
        "        video_path = os.path.join(exercise_path, video_file)\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "        frame_count = 0\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break  # stop when the video ends\n",
        "\n",
        "            # Convert colors\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            results = pose.process(frame_rgb)\n",
        "\n",
        "            if results.pose_landmarks:\n",
        "                # Extract the (x, y, z) for all landmarks\n",
        "                landmarks = []\n",
        "                for lm in results.pose_landmarks.landmark:\n",
        "                    landmarks.extend([lm.x, lm.y, lm.z])\n",
        "\n",
        "                # Add exercise label + frame data\n",
        "                landmarks.append(exercise_name)\n",
        "                data.append(landmarks)\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "        cap.release()\n",
        "        print(f\" Processed {video_file} from {exercise_name}\")\n",
        "\n",
        "# Create DataFrame\n",
        "cols = [f\"x{i}\" for i in range(33)] + [f\"y{i}\" for i in range(33)] + [f\"z{i}\" for i in range(33)] + [\"label\"]\n",
        "df = pd.DataFrame(data, columns=cols)\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"pose_landmarks_dataset.csv\", index=False)\n",
        "print(\"📁 Dataset saved successfully as 'pose_landmarks_dataset.csv'!\")\n",
        "print(df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "5NV31QtV1u2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"pose_landmarks_dataset.csv\")\n",
        "print(\" Data loaded successfully!\")\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "H0tEA3VfnzAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate features and labels\n",
        "X = df.drop(\"label\", axis=1).values\n",
        "y = df[\"label\"].values\n",
        "\n",
        "# Encode labels (e.g., squat → 0, push-up → 1, etc.)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Scale the features (very important for speeding up training)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Split into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\" Data ready for training!\")\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "HY0wwR4an1Vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔧 Improved Model\n",
        "class ExerciseClassifierImproved(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(ExerciseClassifierImproved, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "input_size = X_train.shape[1]\n",
        "num_classes = len(np.unique(y))\n",
        "model = ExerciseClassifierImproved(input_size, num_classes)"
      ],
      "metadata": {
        "id": "gZXy80uZoFqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ensure data is Tensors\n",
        "if not isinstance(X_train, torch.Tensor):\n",
        "    X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "    X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "# Create DataLoaders for training and testing\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "TYfOuy1jo4h2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "num_epochs = 80\n",
        "loss_list = []\n",
        "\n",
        "best_loss = float('inf')\n",
        "patience = 5\n",
        "trigger = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    scheduler.step()\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    loss_list.append(avg_loss)\n",
        "\n",
        "    # Early stopping\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        trigger = 0\n",
        "    else:\n",
        "        trigger += 1\n",
        "        if trigger >= patience:\n",
        "            print(\" Early stopping: Model stopped because performance did not improve\") # Translated comment\n",
        "            break\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "8dnS5gfVKyPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test data\n",
        "# Immediately after training ends\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f\" Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "aJ-7LyM5uyfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on training data\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs_train = model(X_train)\n",
        "    _, predicted_train = torch.max(outputs_train, 1)\n",
        "    correct_train = (predicted_train == y_train).sum().item()\n",
        "    total_train = y_train.size(0)\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "\n",
        "print(f\"🏋️‍♀️ Train Accuracy: {train_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "ILR0xhwvpf4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Make predictions on the test data\n",
        "with torch.no_grad():\n",
        "\n",
        "    outputs = model(X_test_tensor)\n",
        "    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "# Convert numbers to exercise names\n",
        "y_true = le.inverse_transform(y_test.cpu().numpy())\n",
        "y_pred = le.inverse_transform(preds.cpu().numpy())\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred, labels=le.classes_)\n",
        "\n",
        "# Convert to DataFrame for better readability\n",
        "cm_df = pd.DataFrame(cm, index=le.classes_, columns=le.classes_)\n",
        "\n",
        "print(\" Confusion Matrix:\")\n",
        "print(cm_df)\n",
        "\n",
        "# Detailed classification report\n",
        "print(\"\\n Performance Report:\")\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "# Table showing only exercises where the model made mistakes\n",
        "mistakes = []\n",
        "for true_label, pred_label in zip(y_true, y_pred):\n",
        "    if true_label != pred_label:\n",
        "        mistakes.append((true_label, pred_label))\n",
        "\n",
        "if mistakes:\n",
        "    mistake_df = pd.DataFrame(mistakes, columns=[\"True Exercise\", \"Model Prediction\"])\n",
        "    print(\"\\n Exercises with mistakes:\")\n",
        "    print(mistake_df.value_counts().reset_index(name=\"Number of Errors\"))\n",
        "else:\n",
        "    print(\" The model did not make any mistakes!\")"
      ],
      "metadata": {
        "id": "jXzpMoVp6oxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  PLOT LEARNING CURVE\n",
        "plt.plot(loss_list)\n",
        "plt.title(\"Learning Curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VTDEFzViyMHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "#  Create a folder to save inside Google Colab\n",
        "save_dir = \"/content/trained_model\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "#  Save model and component files\n",
        "torch.save(model.state_dict(), os.path.join(save_dir, \"exercise_model.pth\"))\n",
        "joblib.dump(le, os.path.join(save_dir, \"label_encoder.pkl\"))\n",
        "joblib.dump(scaler, os.path.join(save_dir, \"scaler.pkl\"))\n",
        "\n",
        "print(\" Model and preprocessing objects saved successfully in:\", save_dir)\n",
        "\n",
        "#  Download the files to your local machine\n",
        "files.download(os.path.join(save_dir, \"exercise_model.pth\"))\n",
        "files.download(os.path.join(save_dir, \"label_encoder.pkl\"))\n",
        "files.download(os.path.join(save_dir, \"scaler.pkl\"))"
      ],
      "metadata": {
        "id": "tPVLvLcQ6lTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "z2IV5XMERSrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh /content/"
      ],
      "metadata": {
        "id": "JmnEo4ffRgyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# The name of the video as it appeared to you\n",
        "video_path = \"document_6046617532410893975.mp4\"\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(\" Failed to open the video\")\n",
        "else:\n",
        "    print(\" Video opened successfully\")\n",
        "\n",
        "count = 0\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    count += 1\n",
        "\n",
        "print(f\" Number of frames read: {count}\")\n",
        "cap.release()"
      ],
      "metadata": {
        "id": "D04aJYeVsbkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the tools\n",
        "le = joblib.load(\"trained_model/label_encoder.pkl\")\n",
        "scaler = joblib.load(\"trained_model/scaler.pkl\")\n",
        "\n",
        "# Load the model\n",
        "input_size = scaler.mean_.shape[0]\n",
        "num_classes = len(le.classes_)\n",
        "\n",
        "model = ExerciseClassifierImproved(input_size, num_classes)\n",
        "model.load_state_dict(torch.load(\"trained_model/exercise_model.pth\", map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "print(\" Model loaded successfully!\")\n",
        "\n",
        "# Setup Mediapipe\n",
        "mp_pose = mp.solutions.pose\n",
        "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
        "\n",
        "# Test the video\n",
        "video_path = \"document_6046617532410893975.mp4\"\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "if not cap.isOpened():\n",
        "    print(\" Failed to open the video\")\n",
        "else:\n",
        "    print(\" Video opened successfully\")\n",
        "\n",
        "predictions = []\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = pose.process(image_rgb)\n",
        "\n",
        "    if results.pose_landmarks:\n",
        "        landmarks = []\n",
        "        for lm in results.pose_landmarks.landmark:\n",
        "            landmarks.extend([lm.x, lm.y, lm.z])\n",
        "        landmarks = np.array(landmarks).reshape(1, -1)\n",
        "        landmarks_scaled = scaler.transform(landmarks)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = torch.tensor(landmarks_scaled, dtype=torch.float32)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            label = le.inverse_transform([predicted.item()])[0]\n",
        "            predictions.append(label)\n",
        "\n",
        "cap.release()\n",
        "\n",
        "if predictions:\n",
        "    final_label = Counter(predictions).most_common(1)[0][0]\n",
        "    print(f\" The predicted exercise for the video is: {final_label}\")\n",
        "else:\n",
        "    print(\" No clear body points were detected in the video.\")"
      ],
      "metadata": {
        "id": "8FpAn0-HSr-C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}